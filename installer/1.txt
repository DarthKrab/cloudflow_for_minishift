Name:               cloudflow-strimzi-zookeeper-0
Namespace:          cloudflow
Priority:           0
PriorityClassName:  <none>
Node:               <none>
Labels:             app.kubernetes.io/managed-by=helm
                    app.kubernetes.io/name=cloudflow-operator
                    app.kubernetes.io/part-of=cloudflow
                    app.kubernetes.io/version=1.3.0
                    cloudflow.lightbend.com/build-number=1.3.0
                    cloudflow.lightbend.com/release-version=1.2.0
                    controller-revision-hash=cloudflow-strimzi-zookeeper-5c94b7844c
                    statefulset.kubernetes.io/pod-name=cloudflow-strimzi-zookeeper-0
                    strimzi.io/cluster=cloudflow-strimzi
                    strimzi.io/kind=Kafka
                    strimzi.io/name=cloudflow-strimzi-zookeeper
Annotations:        openshift.io/scc=restricted
                    strimzi.io/cluster-ca-cert-generation=0
                    strimzi.io/generation=0
Status:             Pending
IP:                 
Controlled By:      StatefulSet/cloudflow-strimzi-zookeeper
Containers:
  zookeeper:
    Image:      strimzi/kafka:0.13.0-kafka-2.3.0
    Port:       9404/TCP
    Host Port:  0/TCP
    Command:
      /opt/kafka/zookeeper_run.sh
    Limits:
      memory:  3Gi
    Requests:
      memory:   1Gi
    Liveness:   exec [/opt/kafka/zookeeper_healthcheck.sh] delay=15s timeout=5s period=10s #success=1 #failure=3
    Readiness:  exec [/opt/kafka/zookeeper_healthcheck.sh] delay=15s timeout=5s period=10s #success=1 #failure=3
    Environment:
      ZOOKEEPER_NODE_COUNT:          3
      ZOOKEEPER_METRICS_ENABLED:     true
      STRIMZI_KAFKA_GC_LOG_ENABLED:  true
      DYNAMIC_HEAP_FRACTION:         0.75
      DYNAMIC_HEAP_MAX:              2147483648
      ZOOKEEPER_CONFIGURATION:       autopurge.purgeInterval=1
tickTime=2000
initLimit=5
syncLimit=2

    Mounts:
      /opt/kafka/custom-config/ from zookeeper-metrics-and-logging (rw)
      /var/lib/zookeeper from data (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cloudflow-strimzi-zookeeper-token-gcbpr (ro)
  tls-sidecar:
    Image:       strimzi/kafka:0.13.0-kafka-2.3.0
    Ports:       2888/TCP, 3888/TCP, 2181/TCP
    Host Ports:  0/TCP, 0/TCP, 0/TCP
    Command:
      /opt/stunnel/zookeeper_stunnel_run.sh
    Liveness:   exec [/opt/stunnel/stunnel_healthcheck.sh 2181] delay=15s timeout=5s period=10s #success=1 #failure=3
    Readiness:  exec [/opt/stunnel/stunnel_healthcheck.sh 2181] delay=15s timeout=5s period=10s #success=1 #failure=3
    Environment:
      TLS_SIDECAR_LOG_LEVEL:  notice
      ZOOKEEPER_NODE_COUNT:   3
    Mounts:
      /etc/tls-sidecar/cluster-ca-certs/ from cluster-ca-certs (rw)
      /etc/tls-sidecar/zookeeper-nodes/ from zookeeper-nodes (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from cloudflow-strimzi-zookeeper-token-gcbpr (ro)
Conditions:
  Type           Status
  PodScheduled   False 
Volumes:
  data:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  data-cloudflow-strimzi-zookeeper-0
    ReadOnly:   false
  zookeeper-metrics-and-logging:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      cloudflow-strimzi-zookeeper-config
    Optional:  false
  zookeeper-nodes:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cloudflow-strimzi-zookeeper-nodes
    Optional:    false
  cluster-ca-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cloudflow-strimzi-cluster-ca-cert
    Optional:    false
  cloudflow-strimzi-zookeeper-token-gcbpr:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  cloudflow-strimzi-zookeeper-token-gcbpr
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  <none>
Tolerations:     dedicated=StrimziKafka:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  3m (x236 over 38m)  default-scheduler  0/1 nodes are available: 1 node(s) didn't match node selector.
